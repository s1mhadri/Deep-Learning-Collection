{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mntVzX-KkTMw"
   },
   "source": [
    "# **Deep Convolutional Generative Adversarial Network (DC-GAN):**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUxeV8eY3qsO",
    "tags": []
   },
   "source": [
    "### Import Import TensorFlow and other libraries\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5YMUoIHY3w6G"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZuZj95fJ39FP",
    "outputId": "83ff8940-fb83-4284-f236-f9ab7d86dd49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in d:\\ml_dl_cv\\project_env\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: pillow in d:\\ml_dl_cv\\project_env\\lib\\site-packages (from imageio) (8.3.2)\n",
      "Requirement already satisfied: numpy in d:\\ml_dl_cv\\project_env\\lib\\site-packages (from imageio) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "# To generate GIFs for illustration\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GwD5GaM84AtH"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMXQ85DR4DlX"
   },
   "source": [
    "### Load and prepare the dataset\n",
    "\n",
    "The generator will generate handwritten digits resembling the MNIST data.\n",
    "\n",
    "Can use other available datasets. [tensorflow_datasets](https://www.tensorflow.org/datasets/datasets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14Bcb0jq60Hf",
    "outputId": "150e0626-5b1c-4fb6-b93c-b50bb49ccb29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "11501568/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ujQOxSnZ62fO"
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5       # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UJHsgGAk6454"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0XfVAF_X66U2"
   },
   "outputs": [],
   "source": [
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOP8x5Eu7B9Z"
   },
   "source": [
    "## Create the models\n",
    "\n",
    "Both the generator and discriminator are defined using the [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XDMtsqY7DQ3"
   },
   "source": [
    "### The Generator\n",
    "\n",
    "The generator uses `tf.keras.layers.Conv2DTranspose` layers to produce an image from an input noise vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kTJNxJgP7_v2"
   },
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    # Fully connected layer: model.add(layers.Dense(#*#*#, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    # Batch norm\n",
    "    model.add(layers.BatchNormalization())\n",
    "    # Activation function\n",
    "    model.add(layers.LeakyReLU())\n",
    "    # Reshape\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "\n",
    "    # Layer 2: Hint use layers.Conv2DTranspose with 5x5 kernels and appropriate stride\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)      # Note: None is the batch size\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    #Layer4\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Asj3IVEg905a"
   },
   "source": [
    "Use the (untrained) generator to create an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "feQZ-lJC91tW",
    "outputId": "967c1588-080a-4fb2-cfa7-4ddf296fce5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29bd63d4a60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Xmmb04o95iz"
   },
   "source": [
    "### The Discriminator\n",
    "\n",
    "The discriminator is a CNN-based image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cPtnRu5G96Sm"
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    # Layer 1: use layers.Conv2D with 5x5 kernels and appropriate stride\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
    "    # Activation function\n",
    "    model.add(layers.LeakyReLU())\n",
    "    # Dropout\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Layer 2: Conv2D\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    # Activation function\n",
    "    model.add(layers.LeakyReLU())\n",
    "    # Dropout\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Layer 3: Fully connected layer, output dimension must be 1\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkHjQFuX-9pe"
   },
   "source": [
    "Use the (untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CGrJESq-_sW",
    "outputId": "27b3a948-e30e-484b-9e27-34a69cca02b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00093413]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpZVwskn_C7e"
   },
   "source": [
    "## loss and optimizers\n",
    "\n",
    "Define loss functions and optimizers for both models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XV6so7s2_HCG"
   },
   "outputs": [],
   "source": [
    "# Cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5pIPcDWAVve"
   },
   "source": [
    "### Discriminator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "f2WIIBvLA_A1"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZU44oU0sBFx3"
   },
   "source": [
    "### Generator loss\n",
    "The generator's loss quantifies how well it was able to trick the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5-99MaPHBIX-"
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    generator_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    return generator_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nu1nNZ6rEOYd"
   },
   "source": [
    "The discriminator and the generator optimizers are different since both networks are trained separately. Experiment with the learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4qcuMJOYEPr1"
   },
   "outputs": [],
   "source": [
    "#learning rate of 1e-4\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5F29ay2Dzgh"
   },
   "source": [
    "### Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RFHeBD5lD5Z9"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrlCVZutD6wX"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "smzVTAp1D8vV"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 60\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16 # For visualization\n",
    "\n",
    "# We will reuse this noise_vector overtime\n",
    "# to visualize progress in the animated GIF\n",
    "noise_vector = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwbo6eQLEGpm"
   },
   "source": [
    "The training loop begins with generator receiving a random vector as input. That vector will be used to produce an image. The discriminator then classifies real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients used to update the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CYNp18ugFVyI"
   },
   "outputs": [],
   "source": [
    "# 'tf.function' annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise_vector = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Generator output\n",
    "        generated_images = generator(noise_vector, training=True)\n",
    "\n",
    "        # Discriminator output\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        # Loss functions\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # Gradients\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables) \n",
    "\n",
    "    # Update both networks\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7Go3wxFZG7be"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        # Produce images for the GIF\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch + 1, noise_vector)\n",
    "\n",
    "        # Save the model every 20 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           noise_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEfIpOdgG_G9"
   },
   "source": [
    "### Generate and save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "g2Utgnx5HA2V"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # All layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DeQczr2HDD9"
   },
   "source": [
    "## Train the model\n",
    "Train the generator and discriminator simultaneously. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate).\n",
    "\n",
    "At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "_eQcxFIFHJoF",
    "outputId": "d975dcd6-b444-4393-c66d-2c84145d2cfc"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-de5fdd82c318>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Produce images for the GIF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML_DL_CV\\project_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML_DL_CV\\project_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML_DL_CV\\project_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML_DL_CV\\project_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\ML_DL_CV\\project_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML_DL_CV\\project_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SKdS1TKHMbU"
   },
   "source": [
    "**Restore the latest checkpoint.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bp2IoZ2wHPa1",
    "outputId": "8baca107-fa30-4e01-996a-39a43985ff2c"
   },
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWldh9PxHQ9W"
   },
   "source": [
    "## Create a GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52O9ZFnAHSZ-"
   },
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "    return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "3cO4mviuHUPt",
    "outputId": "0c177f73-e4b7-47db-ba2f-dfc18eedfaa5"
   },
   "outputs": [],
   "source": [
    "display_image(EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_uoQPxdHV4t"
   },
   "source": [
    "Use imageio to create an animated gif using the images saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-JQiFzeHXZ1"
   },
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('image*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    last = -1\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frame = 8*(i**0.25)\n",
    "        if round(frame) > round(last):\n",
    "            last = frame\n",
    "        else:\n",
    "            continue\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "\n",
    "\n",
    "import IPython\n",
    "if IPython.version_info > (6,2,0,''):\n",
    "    display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTCG8bdiHZY9"
   },
   "source": [
    "In Colab, download the animation with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "bviCMXpwHahP",
    "outputId": "0f91878a-e300-4e1a-bed0-e80be19b4918"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download(anim_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "4_DC_GAN_Exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
